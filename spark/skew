# Fighting data skew in hash joins

It is worth to repartition data before join - as effort to reduce skew. E.g. like this:

```
val partitions = 1000
val rep = df.withColumn("random", rand()).repartition(partitions, 'random).drop("random")
```

It's because how JOIN works.

Every HashJoin (default) has two phases - shuffle and map. In the shuffle phase ensures that hashes of join keys are in one partition from both datasets.
If data in terms of join keys are not uniformly distributed, then executors with small partitions will be finished with key hashing sooner and we end up
with the last executor hashing the last biggest partition.

So if we reduce the skew in the very beginning as suggested, then the first phase of JOIN will be fast and we don't need to do any "sharding".

We can however perform skew reduction & join in one step:

```
    val shards = 60
    val brandMappingSharded = brandMapping
      .withColumn("shard", typedLit[Seq[Int]](0 until shards))
      .select('normalized, 'master, explode('shard) as "shard")

    p2p
      .withColumn("normalized", transformBrand(rawBrand))
      .drop("brand")
      .withColumn("shard", pmod((rand() * Int.MaxValue) cast LongType, lit(shards)) cast IntegerType)
      .join(brandMappingSharded, List("normalized", "shard"), "left_outer")
      .withColumnRenamed("master", "brand")
      .filter('shard.isNotNull)  // mega important
      .drop("normalized", "shard")
```


```
implicit class DatasetOps[T](fst: Dataset[T]) {
    val shards = 60
    private lazy val fstSharded = fst.withColumn("shard", explode(typedLit[Seq[Int]](0 until shards)))

//    def shardedJoinWith[U](snd: Dataset[U], condition: Column)
//      (implicit spark: SparkSession): Dataset[(T, U)] = {
//      import spark.implicits._
//
//      val sndSharded = snd.withColumn("shard", pmod((rand() * Int.MaxValue) cast LongType, lit(shards)) cast IntegerType)
//      fstSharded
//        .joinWith(sndSharded, condition && fstSharded("shard") == sndSharded("shard"), "left_outer")
//        .filter(fstSharded("shard").isNotNull && sndSharded("shard").isNotNull)
//        .drop(fstSharded("shard"))
//        .drop(sndSharded("shard"))
//        .as[(T, U)]
//    }

    def shardedJoin[U](snd: Dataset[U], condition: Column)(implicit spark: SparkSession): DataFrame = {
      val sndSharded = snd.withColumn("shard2", pmod((rand() * Int.MaxValue) cast LongType, lit(shards)) cast IntegerType)
      import spark.implicits._
      fstSharded
        .join(sndSharded, (fstSharded("shard") === sndSharded("shard2")) && condition, "left_outer")
        .filter($"shard".isNotNull && $"shard2".isNotNull)
        .drop("shard", "shard2")
    }

    def shardedAntiJoin[U](snd: Dataset[U], condition: Column)(implicit spark: SparkSession): DataFrame = {
      val sndSharded = snd.withColumn("shard", pmod((rand() * Int.MaxValue) cast LongType, lit(shards)) cast IntegerType)
      import spark.implicits._
      fstSharded
        .join(sndSharded, (fstSharded("shard") =!= sndSharded("shard")) || condition, "leftanti")
        .filter(fstSharded("shard").isNotNull)
        .drop("shard")
    }
  }
```
